{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7b4db2",
   "metadata": {},
   "source": [
    "# Code Documentation of CheckThat! Subtask 4b Neural Representation learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3355382",
   "metadata": {},
   "source": [
    "## Base-setup trials\n",
    "\n",
    "These trials employ word2vec without and with spaCy preprocessing. Reproduce the base-setup results here by running the following cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ffb07d",
   "metadata": {},
   "source": [
    "Approach with Word2Vec without preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693ec5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding queries (Word2Vec) …\n",
      "Computing metrics for Word2Vec …\n",
      "=== Word2Vec Train Metrics ===\n",
      "MRR@1   : 0.0591\n",
      "MRR@5   : 0.0845\n",
      "MRR@10  : 0.0902\n",
      "Recall@5: 0.1299\n",
      "Recall@10: 0.1737\n",
      "MedianRank: 209.0000\n",
      "=== Word2Vec Dev Metrics ===\n",
      "MRR@1   : 0.0464\n",
      "MRR@5   : 0.0737\n",
      "MRR@10  : 0.0800\n",
      "Recall@5: 0.1193\n",
      "Recall@10: 0.1686\n",
      "MedianRank: 211.5000\n",
      "=== Word2Vec Test Metrics ===\n",
      "MRR@1   : 0.0477\n",
      "MRR@5   : 0.0674\n",
      "MRR@10  : 0.0711\n",
      "Recall@5: 0.1010\n",
      "Recall@10: 0.1300\n",
      "MedianRank: 404.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------- REPRODUCIBILITY ------------------------------------------------\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# ---------------- CONFIG --------------------------------------------------------\n",
    "exp = {\n",
    "    \"experiment_name\":   \"word2vec_no_preprocessing\",\n",
    "    \"encoder_model\":     \"-\",\n",
    "    \"query_field\":       \"-\",\n",
    "    \"normalize\":         \"-\",\n",
    "    \"fine_tune\":         \"-\",\n",
    "    \"epochs\":            \"-\",\n",
    "    \"batch_size\":        \"-\",\n",
    "    \"lr\":                \"-\",\n",
    "    \"use_hard_negatives\": \"-\"\n",
    "}\n",
    "\n",
    "RUN_ID   = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "SAVE_DIR = f\"../models/{exp['experiment_name']}_{RUN_ID}\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "DATA_DIR = \"../../data\"\n",
    "OUT_CSV  = \"../experiment_results/clef_neural_rep_exp_results_with_testdata.csv\"\n",
    "\n",
    "# ---------------- LOAD DATA ----------------------------------------------------\n",
    "df_collection  = pd.read_pickle(f\"{DATA_DIR}/subtask4b_collection_data.pkl\")\n",
    "df_query_train = pd.read_csv(f\"{DATA_DIR}/subtask4b_query_tweets_train.tsv\", sep=\"\\t\")\n",
    "df_query_dev   = pd.read_csv(f\"{DATA_DIR}/subtask4b_query_tweets_dev.tsv\",   sep=\"\\t\")\n",
    "df_query_test  = pd.read_csv(f\"{DATA_DIR}/subtask4b_query_tweets_test_gold.tsv\", sep=\"\\t\")  # TEST DATA\n",
    "\n",
    "corpus = df_collection[['title', 'abstract']].apply(\n",
    "    lambda x: f\"{x['title']} {x['abstract']}\", axis=1\n",
    ").tolist()\n",
    "cord_uids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "# ---------------- TOKENIZATION -------------------------------------------------\n",
    "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "\n",
    "# ---------------- WORD2VEC TRAINING --------------------------------------------\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100, \n",
    "    window=5, \n",
    "    min_count=2,\n",
    "    workers=4,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# ---------------- EMBEDDINGS ---------------------------------------------------\n",
    "def get_avg_embedding(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "doc_embeddings = np.array([\n",
    "    get_avg_embedding(tokens, w2v_model) for tokens in tokenized_corpus\n",
    "])\n",
    "\n",
    "def encode_queries_w2v(queries, model):\n",
    "    tokenized = [q.lower().split() for q in queries]\n",
    "    return np.array([\n",
    "        get_avg_embedding(tokens, model) for tokens in tokenized\n",
    "    ])\n",
    "\n",
    "print(\"Encoding queries (Word2Vec) …\")\n",
    "q_emb_train_w2v = encode_queries_w2v(df_query_train['tweet_text'].tolist(), w2v_model)\n",
    "q_emb_dev_w2v   = encode_queries_w2v(df_query_dev['tweet_text'].tolist(),   w2v_model)\n",
    "q_emb_test_w2v  = encode_queries_w2v(df_query_test['tweet_text'].tolist(),  w2v_model)  # TEST\n",
    "\n",
    "# ---------------- METRICS ------------------------------------------------------\n",
    "def compute_metrics(q_emb, doc_emb, gt_ids, doc_ids):\n",
    "    sims = cosine_similarity(q_emb, doc_emb)\n",
    "    ranks = []\n",
    "    rr1 = []\n",
    "    rr5 = []\n",
    "    rr10 = []\n",
    "    for i, row in enumerate(sims):\n",
    "        order = np.argsort(-row)  # sort descending\n",
    "        gt_idx = doc_ids.index(gt_ids[i])\n",
    "        rank = int(np.where(order == gt_idx)[0][0]) + 1\n",
    "        ranks.append(rank)\n",
    "        rr1.append(1.0/rank if rank <= 1 else 0.0)\n",
    "        rr5.append(1.0/rank if rank <= 5 else 0.0)\n",
    "        rr10.append(1.0/rank if rank <= 10 else 0.0)\n",
    "    ranks = np.array(ranks)\n",
    "    metrics = {\n",
    "        \"MRR@1\":       float(np.mean(rr1)),\n",
    "        \"MRR@5\":       float(np.mean(rr5)),\n",
    "        \"MRR@10\":      float(np.mean(rr10)),\n",
    "        \"Recall@5\":    float((ranks <= 5).mean()),\n",
    "        \"Recall@10\":   float((ranks <= 10).mean()),\n",
    "        \"MedianRank\":  float(np.median(ranks))\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Computing metrics for Word2Vec …\")\n",
    "train_metrics_w2v = compute_metrics(\n",
    "    q_emb_train_w2v, doc_embeddings,\n",
    "    df_query_train['cord_uid'].tolist(),\n",
    "    cord_uids\n",
    ")\n",
    "dev_metrics_w2v = compute_metrics(\n",
    "    q_emb_dev_w2v, doc_embeddings,\n",
    "    df_query_dev['cord_uid'].tolist(),\n",
    "    cord_uids\n",
    ")\n",
    "test_metrics_w2v = compute_metrics( # TEST DATA\n",
    "    q_emb_test_w2v, doc_embeddings,\n",
    "    df_query_test['cord_uid'].tolist(),\n",
    "    cord_uids\n",
    ")\n",
    "\n",
    "# ---------------- PRINT METRICS ------------------------------------------------\n",
    "print(\"=== Word2Vec Train Metrics ===\")\n",
    "for k, v in train_metrics_w2v.items():\n",
    "    print(f\"{k:8s}: {v:.4f}\")\n",
    "print(\"=== Word2Vec Dev Metrics ===\")\n",
    "for k, v in dev_metrics_w2v.items():\n",
    "    print(f\"{k:8s}: {v:.4f}\")#\n",
    "print(\"=== Word2Vec Test Metrics ===\")\n",
    "for k, v in test_metrics_w2v.items():\n",
    "    print(f\"{k:8s}: {v:.4f}\")\n",
    "\n",
    "# ---------------- Optional: LOGGING ---------------------------------------------\n",
    "# log_w2v = {\n",
    "#     **exp,\n",
    "#     **{f\"train_{k}\": v for k, v in train_metrics_w2v.items()},\n",
    "#     **{f\"dev_{k}\":   v for k, v in dev_metrics_w2v.items()},\n",
    "#     **{f\"test_{k}\":  v for k, v in test_metrics_w2v.items()},\n",
    "#     \"timestamp\": datetime.now().isoformat()\n",
    "# }\n",
    "\n",
    "# pd.DataFrame([log_w2v]).to_csv(\n",
    "#     OUT_CSV, mode=\"a\",\n",
    "#     header=not os.path.exists(OUT_CSV),\n",
    "#     index=False\n",
    "# )\n",
    "\n",
    "# print(\"Logged Word2Vec (no preprocessing) results to\", OUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5020c735",
   "metadata": {},
   "source": [
    "Approach with Word2Vec with spacy preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56396307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing corpus …\n",
      "Training Word2Vec model …\n",
      "Computing document embeddings …\n",
      "Encoding queries (Word2Vec) …\n",
      "Computing metrics for Word2Vec with spaCy-preprocessing …\n",
      "=== Word2Vec (spaCy-preprocessed) Train Metrics ===\n",
      "MRR@1   : 0.1080\n",
      "MRR@5   : 0.1512\n",
      "MRR@10  : 0.1603\n",
      "Recall@5: 0.2272\n",
      "Recall@10: 0.2948\n",
      "MedianRank: 52.0000\n",
      "=== Word2Vec (spaCy-preprocessed) Dev Metrics ===\n",
      "MRR@1   : 0.1107\n",
      "MRR@5   : 0.1455\n",
      "MRR@10  : 0.1551\n",
      "Recall@5: 0.2121\n",
      "Recall@10: 0.2850\n",
      "MedianRank: 55.0000\n",
      "=== Word2Vec (spaCy-preprocessed) Test Metrics ===\n",
      "MRR@1   : 0.0740\n",
      "MRR@5   : 0.1064\n",
      "MRR@10  : 0.1137\n",
      "Recall@5: 0.1632\n",
      "Recall@10: 0.2192\n",
      "MedianRank: 97.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "import os\n",
    "import spacy\n",
    "\n",
    "# ---------------- REPRODUCIBILITY ------------------------------------------------\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# ---------------- CONFIG --------------------------------------------------------\n",
    "exp = {\n",
    "    \"experiment_name\":   \"word2vec_spacy_preprocessing\",\n",
    "    \"encoder_model\":     \"-\",\n",
    "    \"query_field\":       \"-\",\n",
    "    \"normalize\":         \"-\",\n",
    "    \"fine_tune\":         \"-\",\n",
    "    \"epochs\":            \"-\",\n",
    "    \"batch_size\":        \"-\",\n",
    "    \"lr\":                \"-\",\n",
    "    \"use_hard_negatives\": \"-\"\n",
    "}\n",
    "\n",
    "# ---------------- PREPROCESSING SPACY -------------------------------------------\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Tokenize input text, lemmatize each token, lowercase it,\n",
    "    and filter out tokens that are either alphabetic or numeric (no stopwords).\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for tok in doc:\n",
    "        if (tok.is_alpha or tok.like_num) and not tok.is_stop:\n",
    "            tokens.append(tok.lemma_.lower())\n",
    "    return tokens\n",
    "\n",
    "corpus = (\n",
    "    df_collection[['title', 'abstract']]\n",
    "    .apply(lambda x: f\"{x['title']} {x['abstract']}\", axis=1)\n",
    "    .tolist()\n",
    ")\n",
    "cord_uids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "# Tokenization\n",
    "print(\"Tokenizing corpus …\")\n",
    "tokenized_corpus = [preprocess(doc) for doc in corpus]\n",
    "\n",
    "# ---------------- WORD2VEC TRAINING --------------------------------------------\n",
    "print(\"Training Word2Vec model …\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_corpus,\n",
    "    vector_size=100, \n",
    "    window=5, \n",
    "    min_count=2, \n",
    "    workers=4,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# ---------------- EMBEDDINGS ---------------------------------------------------\n",
    "def get_avg_embedding(tokens, model):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "print(\"Computing document embeddings …\")\n",
    "doc_embeddings = [get_avg_embedding(doc, w2v_model) for doc in tokenized_corpus]\n",
    "doc_embeddings = np.array(doc_embeddings)\n",
    "\n",
    "def encode_queries_w2v(queries, model):\n",
    "    tokenized = [preprocess(q) for q in queries]\n",
    "    return np.array([get_avg_embedding(tokens, model) for tokens in tokenized])\n",
    "\n",
    "print(\"Encoding queries (Word2Vec) …\")\n",
    "q_emb_train_w2v = encode_queries_w2v(df_query_train['tweet_text'].tolist(), w2v_model)\n",
    "q_emb_dev_w2v   = encode_queries_w2v(df_query_dev['tweet_text'].tolist(), w2v_model)\n",
    "q_emb_test_w2v   = encode_queries_w2v(df_query_test['tweet_text'].tolist(), w2v_model) # TEST DATA\n",
    "\n",
    "# ---------------- METRICS ------------------------------------------------------\n",
    "def compute_metrics(q_emb, doc_emb, gt_ids, doc_ids):\n",
    "    sims = cosine_similarity(q_emb, doc_emb)\n",
    "    ranks = []\n",
    "    rr1 = []\n",
    "    rr5 = []\n",
    "    rr10 = []\n",
    "    for i, row in enumerate(sims):\n",
    "        order = np.argsort(-row)  # sort descending\n",
    "        gt_idx = doc_ids.index(gt_ids[i])\n",
    "        rank = int(np.where(order == gt_idx)[0][0]) + 1\n",
    "        ranks.append(rank)\n",
    "        rr1.append(1.0/rank if rank <= 1 else 0.0)\n",
    "        rr5.append(1.0/rank if rank <= 5 else 0.0)\n",
    "        rr10.append(1.0/rank if rank <= 10 else 0.0)\n",
    "    ranks = np.array(ranks)\n",
    "    metrics = {\n",
    "        \"MRR@1\":       float(np.mean(rr1)),\n",
    "        \"MRR@5\":       float(np.mean(rr5)),\n",
    "        \"MRR@10\":      float(np.mean(rr10)),\n",
    "        \"Recall@5\":    float((ranks <= 5).mean()),\n",
    "        \"Recall@10\":   float((ranks <= 10).mean()),\n",
    "        \"MedianRank\":  float(np.median(ranks))\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "print(\"Computing metrics for Word2Vec with spaCy-preprocessing …\")\n",
    "train_metrics_w2v = compute_metrics(\n",
    "    q_emb_train_w2v, doc_embeddings,\n",
    "    df_query_train['cord_uid'].tolist(),\n",
    "    cord_uids\n",
    ")\n",
    "dev_metrics_w2v   = compute_metrics(\n",
    "    q_emb_dev_w2v, doc_embeddings,\n",
    "    df_query_dev['cord_uid'].tolist(),\n",
    "    cord_uids\n",
    ")\n",
    "test_metrics_w2v = compute_metrics( # TEST DATA\n",
    "    q_emb_test_w2v, doc_embeddings,\n",
    "    df_query_test['cord_uid'].tolist(),\n",
    "    cord_uids\n",
    ")\n",
    "\n",
    "print(\"=== Word2Vec (spaCy-preprocessed) Train Metrics ===\")\n",
    "for k, v in train_metrics_w2v.items():\n",
    "    print(f\"{k:8s}: {v:.4f}\")\n",
    "print(\"=== Word2Vec (spaCy-preprocessed) Dev Metrics ===\")\n",
    "for k, v in dev_metrics_w2v.items():\n",
    "    print(f\"{k:8s}: {v:.4f}\")\n",
    "print(\"=== Word2Vec (spaCy-preprocessed) Test Metrics ===\")\n",
    "for k, v in test_metrics_w2v.items():\n",
    "    print(f\"{k:8s}: {v:.4f}\")\n",
    "\n",
    "# ---------------- Optional: LOGGING ---------------------------------------------------\n",
    "# log_w2v = {\n",
    "#     **exp,\n",
    "#     **{f\"train_{k}\": v for k, v in train_metrics_w2v.items()},\n",
    "#     **{f\"dev_{k}\":   v for k, v in dev_metrics_w2v.items()},\n",
    "#     **{f\"test_{k}\":  v for k, v in test_metrics_w2v.items()},\n",
    "#     \"timestamp\": datetime.now().isoformat()\n",
    "# }\n",
    "\n",
    "# pd.DataFrame([log_w2v]).to_csv(\n",
    "#     OUT_CSV, mode=\"a\",\n",
    "#     header=not os.path.exists(OUT_CSV),\n",
    "#     index=False\n",
    "# )\n",
    "\n",
    "# print(\"Logged Word2Vec (spaCy-preprocessed) results to\", OUT_CSV)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIR_CLEF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
