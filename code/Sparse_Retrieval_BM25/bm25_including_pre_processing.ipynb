{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfnQvGfvtH1w"
   },
   "source": [
    "# Sparse Retrieval: BM25\n",
    "\n",
    "### CLEF 2025 - CheckThat! Lab  - Task 4 Scientific Web Discourse - Subtask 4b (Scientific Claim Source Retrieval)\n",
    "\n",
    "This notebook enables to get an idea of the power of Sparse Retrieval baseline model for the subtask 4b. It includes the following:\n",
    "- Code to upload data, including:\n",
    "    - code to upload the collection set (CORD-19 academic papers' metadata)\n",
    "    - code to upload the query set (tweets with implicit references to CORD-19 papers)\n",
    "- Code to run a baseline retrieval model (BM25)\n",
    "- Code to evaluate the baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpDCfBMouNAL"
   },
   "source": [
    "# 1) Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1742975967136,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "rQPqDKP_QHFM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from numpy.random import RandomState\n",
    "import random\n",
    "# !pip install rank_bm25\n",
    "from rank_bm25 import BM25Okapi\n",
    "import spacy\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u8N7h9BhQI5m"
   },
   "source": [
    "## 1.a) Import the collection set\n",
    "The collection set contains metadata of CORD-19 academic papers.\n",
    "\n",
    "The preprocessed and filtered CORD-19 dataset is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742975971100,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "2GQI4HcKR6hS"
   },
   "outputs": [],
   "source": [
    "# 1) Download the collection set from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b\n",
    "# 2) Drag and drop the downloaded file to the \"data\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file path\n",
    "PATH_COLLECTION_DATA = '../../data/subtask4b_collection_data.pkl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1742975975524,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "SYBB3UYbMwTA"
   },
   "outputs": [],
   "source": [
    "df_collection = pd.read_pickle(PATH_COLLECTION_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1742975976305,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "4v3lygNOQQSn",
    "outputId": "ee5b9abd-f889-4a4e-ce11-32d2691433cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7718 entries, 162 to 1056448\n",
      "Data columns (total 17 columns):\n",
      " #   Column            Non-Null Count  Dtype         \n",
      "---  ------            --------------  -----         \n",
      " 0   cord_uid          7718 non-null   object        \n",
      " 1   source_x          7718 non-null   object        \n",
      " 2   title             7718 non-null   object        \n",
      " 3   doi               7677 non-null   object        \n",
      " 4   pmcid             4959 non-null   object        \n",
      " 5   pubmed_id         6233 non-null   object        \n",
      " 6   license           7718 non-null   object        \n",
      " 7   abstract          7718 non-null   object        \n",
      " 8   publish_time      7715 non-null   object        \n",
      " 9   authors           7674 non-null   object        \n",
      " 10  journal           6668 non-null   object        \n",
      " 11  mag_id            0 non-null      float64       \n",
      " 12  who_covidence_id  528 non-null    object        \n",
      " 13  arxiv_id          20 non-null     object        \n",
      " 14  label             7718 non-null   object        \n",
      " 15  time              7715 non-null   datetime64[ns]\n",
      " 16  timet             7718 non-null   int64         \n",
      "dtypes: datetime64[ns](1), float64(1), int64(1), object(14)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_collection.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 608
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1742975978238,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "9veNFFGDZRx7",
    "outputId": "5eec7f85-7d20-44d7-8986-a85cb00533d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>mag_id</th>\n",
       "      <th>who_covidence_id</th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>label</th>\n",
       "      <th>time</th>\n",
       "      <th>timet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Professional and Home-Made Face Masks Reduce E...</td>\n",
       "      <td>10.1371/journal.pone.0002618</td>\n",
       "      <td>PMC2440799</td>\n",
       "      <td>18612429</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>BACKGROUND: Governments are preparing for a po...</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>van der Sande, Marianne; Teunis, Peter; Sabel,...</td>\n",
       "      <td>PLoS One</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>umvrwgaw</td>\n",
       "      <td>2008-07-09</td>\n",
       "      <td>1215561600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>PMC</td>\n",
       "      <td>The Failure of R (0)</td>\n",
       "      <td>10.1155/2011/527610</td>\n",
       "      <td>PMC3157160</td>\n",
       "      <td>21860658</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The basic reproductive ratio, R (0), is one of...</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>Li, Jing; Blakeley, Daniel; Smith?, Robert J.</td>\n",
       "      <td>Comput Math Methods Med</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spiud6ok</td>\n",
       "      <td>2011-08-16</td>\n",
       "      <td>1313452800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>918</th>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Pulmonary sequelae in a patient recovered from...</td>\n",
       "      <td>10.4103/0970-2113.99118</td>\n",
       "      <td>PMC3424870</td>\n",
       "      <td>22919170</td>\n",
       "      <td>cc-by-nc-sa</td>\n",
       "      <td>The pandemic of swine flu (H1N1) influenza spr...</td>\n",
       "      <td>2012</td>\n",
       "      <td>Singh, Virendra; Sharma, Bharat Bhushan; Patel...</td>\n",
       "      <td>Lung India</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aclzp3iy</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>1325376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>PMC</td>\n",
       "      <td>What was the primary mode of smallpox transmis...</td>\n",
       "      <td>10.3389/fcimb.2012.00150</td>\n",
       "      <td>PMC3509329</td>\n",
       "      <td>23226686</td>\n",
       "      <td>cc-by</td>\n",
       "      <td>The mode of infection transmission has profoun...</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>Milton, Donald K.</td>\n",
       "      <td>Front Cell Infect Microbiol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ycxyn2a2</td>\n",
       "      <td>2012-11-29</td>\n",
       "      <td>1354147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>PMC</td>\n",
       "      <td>Lessons from the History of Quarantine, from P...</td>\n",
       "      <td>10.3201/eid1902.120312</td>\n",
       "      <td>PMC3559034</td>\n",
       "      <td>23343512</td>\n",
       "      <td>no-cc</td>\n",
       "      <td>In the new millennium, the centuries-old strat...</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>Tognotti, Eugenia</td>\n",
       "      <td>Emerg Infect Dis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxe95qy9</td>\n",
       "      <td>2013-02-03</td>\n",
       "      <td>1359849600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cord_uid source_x                                              title  \\\n",
       "162   umvrwgaw      PMC  Professional and Home-Made Face Masks Reduce E...   \n",
       "611   spiud6ok      PMC                               The Failure of R (0)   \n",
       "918   aclzp3iy      PMC  Pulmonary sequelae in a patient recovered from...   \n",
       "993   ycxyn2a2      PMC  What was the primary mode of smallpox transmis...   \n",
       "1053  zxe95qy9      PMC  Lessons from the History of Quarantine, from P...   \n",
       "\n",
       "                               doi       pmcid pubmed_id      license  \\\n",
       "162   10.1371/journal.pone.0002618  PMC2440799  18612429        cc-by   \n",
       "611            10.1155/2011/527610  PMC3157160  21860658        cc-by   \n",
       "918        10.4103/0970-2113.99118  PMC3424870  22919170  cc-by-nc-sa   \n",
       "993       10.3389/fcimb.2012.00150  PMC3509329  23226686        cc-by   \n",
       "1053        10.3201/eid1902.120312  PMC3559034  23343512        no-cc   \n",
       "\n",
       "                                               abstract publish_time  \\\n",
       "162   BACKGROUND: Governments are preparing for a po...   2008-07-09   \n",
       "611   The basic reproductive ratio, R (0), is one of...   2011-08-16   \n",
       "918   The pandemic of swine flu (H1N1) influenza spr...         2012   \n",
       "993   The mode of infection transmission has profoun...   2012-11-29   \n",
       "1053  In the new millennium, the centuries-old strat...   2013-02-03   \n",
       "\n",
       "                                                authors  \\\n",
       "162   van der Sande, Marianne; Teunis, Peter; Sabel,...   \n",
       "611       Li, Jing; Blakeley, Daniel; Smith?, Robert J.   \n",
       "918   Singh, Virendra; Sharma, Bharat Bhushan; Patel...   \n",
       "993                                   Milton, Donald K.   \n",
       "1053                                  Tognotti, Eugenia   \n",
       "\n",
       "                          journal  mag_id who_covidence_id arxiv_id     label  \\\n",
       "162                      PLoS One     NaN              NaN      NaN  umvrwgaw   \n",
       "611       Comput Math Methods Med     NaN              NaN      NaN  spiud6ok   \n",
       "918                    Lung India     NaN              NaN      NaN  aclzp3iy   \n",
       "993   Front Cell Infect Microbiol     NaN              NaN      NaN  ycxyn2a2   \n",
       "1053             Emerg Infect Dis     NaN              NaN      NaN  zxe95qy9   \n",
       "\n",
       "           time       timet  \n",
       "162  2008-07-09  1215561600  \n",
       "611  2011-08-16  1313452800  \n",
       "918  2012-01-01  1325376000  \n",
       "993  2012-11-29  1354147200  \n",
       "1053 2013-02-03  1359849600  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_collection.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAUiDU0xXLBt"
   },
   "source": [
    "## 1.b) Import the query set\n",
    "\n",
    "The query set contains tweets with implicit references to academic papers from the collection set.\n",
    "\n",
    "The preprocessed query set is available on the Gitlab repository here: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1742975982410,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "v8gwkZDSXPsd"
   },
   "outputs": [],
   "source": [
    "# 1) Download the query tweets from the Gitlab repository: https://gitlab.com/checkthat_lab/clef2025-checkthat-lab/-/tree/main/task4/subtask_4b?ref_type=heads\n",
    "# 2) Drag and drop the downloaded file to the \"Files\" section (left vertical menu on Colab)\n",
    "# 3) Modify the path to your local file patPATH_COLLECTION_DATA = '../data/subtask4b_collection_data.pkl'\n",
    "PATH_QUERY_TRAIN_DATA = '../../data/ubtask4b_query_tweets_train.tsv'\n",
    "PATH_QUERY_DEV_DATA = '../../data/subtask4b_query_tweets_dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1742976006985,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "VqxjYq2tYDmE"
   },
   "outputs": [],
   "source": [
    "df_query_train = pd.read_csv(PATH_QUERY_TRAIN_DATA, sep = '\\t')\n",
    "df_query_dev = pd.read_csv(PATH_QUERY_DEV_DATA, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>3qvh482o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>\"Among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>r58aohnu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>I recall early on reading that researchers who...</td>\n",
       "      <td>sts48u9i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>You know you're credible when NIH website has ...</td>\n",
       "      <td>3sr2exq9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Resistance to antifungal medications is a grow...</td>\n",
       "      <td>ybwwmyqy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid\n",
       "0       16  covid recovery: this study from the usa reveal...  3qvh482o\n",
       "1       69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu\n",
       "2       73  I recall early on reading that researchers who...  sts48u9i\n",
       "3       93  You know you're credible when NIH website has ...  3sr2exq9\n",
       "4       96  Resistance to antifungal medications is a grow...  ybwwmyqy"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "szMEK3OkYLvX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oral care in rehabilitation medicine: oral vul...</td>\n",
       "      <td>htlvpvz5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>this study isn't receiving sufficient attentio...</td>\n",
       "      <td>4kfl29ul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>thanks, xi jinping. a reminder that this study...</td>\n",
       "      <td>jtwb17u8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Taiwan - a population of 23 million has had ju...</td>\n",
       "      <td>0w9k8iy1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Obtaining a diagnosis of autism in lower incom...</td>\n",
       "      <td>tiqksd69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid\n",
       "0        0  Oral care in rehabilitation medicine: oral vul...  htlvpvz5\n",
       "1        1  this study isn't receiving sufficient attentio...  4kfl29ul\n",
       "2        2  thanks, xi jinping. a reminder that this study...  jtwb17u8\n",
       "3        3  Taiwan - a population of 23 million has had ju...  0w9k8iy1\n",
       "4        4  Obtaining a diagnosis of autism in lower incom...  tiqksd69"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "aslmTTJQyL2X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12853 entries, 0 to 12852\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   post_id     12853 non-null  int64 \n",
      " 1   tweet_text  12853 non-null  object\n",
      " 2   cord_uid    12853 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 301.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_query_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1742976030778,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "B5X8FwLhLY3u",
    "outputId": "36e21737-8257-4568-8346-0d3e0980ee53"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cord_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>covid recovery: this study from the usa reveal...</td>\n",
       "      <td>3qvh482o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>\"Among 139 clients exposed to two symptomatic ...</td>\n",
       "      <td>r58aohnu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>73</td>\n",
       "      <td>I recall early on reading that researchers who...</td>\n",
       "      <td>sts48u9i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>You know you're credible when NIH website has ...</td>\n",
       "      <td>3sr2exq9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96</td>\n",
       "      <td>Resistance to antifungal medications is a grow...</td>\n",
       "      <td>ybwwmyqy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                         tweet_text  cord_uid\n",
       "0       16  covid recovery: this study from the usa reveal...  3qvh482o\n",
       "1       69  \"Among 139 clients exposed to two symptomatic ...  r58aohnu\n",
       "2       73  I recall early on reading that researchers who...  sts48u9i\n",
       "3       93  You know you're credible when NIH website has ...  3sr2exq9\n",
       "4       96  Resistance to antifungal medications is a grow...  ybwwmyqy"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_query_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1742976032804,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "t6gDlBZnLcdH",
    "outputId": "11cd57d2-a4b7-4b06-a9af-9ba5e29c191b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   post_id     1400 non-null   int64 \n",
      " 1   tweet_text  1400 non-null   object\n",
      " 2   cord_uid    1400 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 32.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_query_dev.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jr_BDzufPmmP"
   },
   "source": [
    "# 2) Running the baseline\n",
    "The following code runs a BM25 and TF-IDF baseline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theoretical Fundamentals of Sparse Retrieval Baseline BM25\n",
    "\n",
    "**BM25 (Okapi)**  \n",
    "$$\n",
    "\\mathrm{score}(q,d)\n",
    "= \\sum_{t \\in T_d \\cap T_q}\n",
    "\\frac{tf_{t,d}}\n",
    "     {\\underbrace{k_1\\Bigl((1-b) + b\\,\\frac{dl_d}{\\mathrm{avgdl}}\\Bigr) + tf_{t,d}}_{\\text{TF saturation + length norm}}}\n",
    "\\;\\times\\;\n",
    "\\underbrace{\\log\\!\\frac{\\lvert D\\rvert - df_t + 0.5}{df_t + 0.5}}_{\\text{RSJ IDF}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $tf_{t,d}$: frequency of term $t$ in document $d$.  \n",
    "- $dl_d$: length of $d$ in tokens; $\\mathrm{avgdl}$: average document length in the collection.  \n",
    "- $df_t$: number of documents containing term $t$; $\\lvert D\\rvert$: total number of documents.  \n",
    "- $k_1 > 0$: term‐frequency saturation parameter (larger $k_1 \\!\\to\\!$ slower saturation).  \n",
    "- $b \\in [0,1]$: length‐normalisation parameter ($b=0$ disables, $b=1$ full normalisation).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### MRR@k (Mean Reciprocal Rank at cutoff *k*)\n",
    "\n",
    "Let *r* be the rank (1-indexed) of the first relevant document for a given query. Then  \n",
    "$$\n",
    "\\mathrm{MRR}@k \\;=\\;\n",
    "\\frac{1}{|Q|}\\sum_{q\\in Q}\n",
    "\\begin{cases}\n",
    "\\dfrac{1}{r}, & r \\le k,\\\\[6pt]\n",
    "0,            & r > k.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **MRR@1**: only counts if the top result is relevant (score = 1 or 0).  \n",
    "- **MRR@5**: uses reciprocal ranks (1, 1/2, 1/3, 1/4, 1/5) for positions 1…5; 0 beyond.  \n",
    "- **MRR@10**: uses reciprocal ranks (1, 1/2, …, 1/10) for positions 1…10.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BM25 Baseline Pipeline Overview**\n",
    "\n",
    "\n",
    "\n",
    "**Preprocessing**  \n",
    "- Applies spaCy lemmatization and lowercasing  \n",
    "- Keeps alphabetic (`tok.is_alpha`) and numeric (`tok.like_num`) tokens  \n",
    "- Removes stopwords (`not tok.is_stop`)  \n",
    "- Ensures tokens like “studies” → “study” and retains numbers (e.g. “2021”)\n",
    "\n",
    "**Corpus Creation**  \n",
    "- Concatenates each document’s `title` + `abstract` into one string → `corpus`  \n",
    "- Extracts `cord_uids` for mapping results back to documents  \n",
    "\n",
    "**Tokenization**  \n",
    "- Transforms every entry in `corpus` via `preprocess()` → `tokenized_corpus` (list of token lists)  \n",
    "\n",
    "**Index Initialization**  \n",
    "- Builds the BM25 index:  \n",
    "  ```python\n",
    "  bm25 = BM25Okapi(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1414,
     "status": "ok",
     "timestamp": 1742976047296,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "jXCC7K_ZPQL2"
   },
   "outputs": [],
   "source": [
    "# Load the small English model, disabling the parser and named-entity recognizer to save memory\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# def preprocess(text):\n",
    "#     \"\"\"\n",
    "#     Tokenize input text, lemmatize each token, lowercase it,\n",
    "#     and filter out non-alphabetic tokens and stopwords.\n",
    "#     \"\"\"\n",
    "#     doc = nlp(text)\n",
    "#     return [\n",
    "#         tok.lemma_.lower() \n",
    "#         for tok in doc \n",
    "#         if tok.is_alpha and not tok.is_stop\n",
    "#     ]\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Tokenize input text, lemmatize each token, lowercase it,\n",
    "    and filter out tokens that are neither pure words nor numbers,\n",
    "    plus remove stopwords.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for tok in doc:\n",
    "        # keep tokens that are alphabetic OR numeric, but not stopwords\n",
    "        if (tok.is_alpha or tok.like_num) and not tok.is_stop:\n",
    "            tokens.append(tok.lemma_.lower())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "# 2) Create the corpus (unchanged)\n",
    "#    - Combine title and abstract into one string per document\n",
    "corpus = (\n",
    "    df_collection[['title', 'abstract']]\n",
    "    .apply(lambda x: f\"{x['title']} {x['abstract']}\", axis=1)\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "#    - Keep track of each document’s unique ID\n",
    "cord_uids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "\n",
    "# 3) Tokenize with lemmatization & initialize BM25\n",
    "#    - Apply our preprocess() to every document\n",
    "tokenized_corpus = [preprocess(doc) for doc in corpus]\n",
    "\n",
    "#    - Build the BM25 index over the lemmatized tokens\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "include authors and journal -->leads to no improvement in MMR@5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the small English model, disabling the parser and named-entity recognizer to save memory\n",
    "# nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# def preprocess(text):\n",
    "#     \"\"\"\n",
    "#     Tokenize input text, lemmatize each token, lowercase it,\n",
    "#     and filter out non-alphabetic tokens and stopwords.\n",
    "#     \"\"\"\n",
    "#     doc = nlp(text)\n",
    "#     return [\n",
    "#         tok.lemma_.lower() \n",
    "#         for tok in doc \n",
    "#         if tok.is_alpha and not tok.is_stop\n",
    "#     ]\n",
    "\n",
    "\n",
    "\n",
    "# # 2) Corpus‑Erstellung mit Title, Abstract, Authors & Journal\n",
    "# def combine_fields(row):\n",
    "#     # Falls authors als Liste vorliegt, zu einem String zusammenfügen\n",
    "#     authors = \", \".join(row['authors']) if isinstance(row['authors'], list) else str(row['authors'])\n",
    "#     journal = str(row['journal']) if not pd.isna(row['journal']) else \"\"\n",
    "#     return f\"{row['title']} {row['abstract']} {authors} {journal}\"\n",
    "\n",
    "# corpus = (\n",
    "#     df_collection[['title','abstract','authors','journal']]\n",
    "#     .apply(combine_fields, axis=1)\n",
    "#     .tolist()\n",
    "# )\n",
    "\n",
    "# cord_uids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "# # 3) Tokenisierung & BM25‑Initialisierung bleibt unverändert\n",
    "# tokenized_corpus = [preprocess(doc) for doc in corpus]\n",
    "# bm25 = BM25Okapi(tokenized_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A global dictionary `_text2bm25top` caches mappings from queries to their top‑k document IDs.  \n",
    "`get_top_cord_uids(query, k=5)` checks `_text2bm25top`; on a cache miss it calls `preprocess(query)`, obtains BM25 scores via `bm25.get_scores()`, selects the top‑k indices with `np.argsort(-scores)[:k]`, looks up the corresponding `cord_uids`, stores the result in `_text2bm25top`, and returns the list of IDs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1742976047304,
     "user": {
      "displayName": "Yavuz",
      "userId": "01318046262282431930"
     },
     "user_tz": -60
    },
    "id": "e8NeJWGYPQZG"
   },
   "outputs": [],
   "source": [
    "# global Cache\n",
    "_text2bm25top = {}\n",
    "\n",
    "def get_top_cord_uids(query, k=10):\n",
    "    # return cached?\n",
    "    if query in _text2bm25top:\n",
    "        return _text2bm25top[query]\n",
    "    # tokenize with Lemmatization\n",
    "    tokenized_query = preprocess(query)\n",
    "    # Score-Calcuation\n",
    "    doc_scores = bm25.get_scores(tokenized_query)\n",
    "    indices    = np.argsort(-doc_scores)[:k]\n",
    "    topk_uids  = [cord_uids[i] for i in indices]\n",
    "    # Fill cache\n",
    "    _text2bm25top[query] = topk_uids\n",
    "    return topk_uids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk4-BqEtTgUj"
   },
   "outputs": [],
   "source": [
    "# train and dev queries\n",
    "df_query_train['bm25_topk'] = df_query_train['tweet_text'] \\\n",
    "                                .apply(lambda txt: get_top_cord_uids(txt, k=10))\n",
    "df_query_dev  ['bm25_topk'] = df_query_dev  ['tweet_text'] \\\n",
    "                                .apply(lambda txt: get_top_cord_uids(txt, k=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVKBlTCZUMSc"
   },
   "source": [
    "# 3) Evaluating the baseline\n",
    "The following code evaluates the BM25 retrieval baseline on the train, dev and test query set using the Mean Reciprocal Rank score (MRR@1, MRR@5, MRR@10, Recall@5, Recall@10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: {'MRR@1': np.float64(0.5584688399595426), 'MRR@5': np.float64(0.610892398661791), 'MRR@10': np.float64(0.6170860919382666), 'Recall@5': 0.6933011748229985, 'Recall@10': 0.7395938691356103}\n",
      "Dev:   {'MRR@1': np.float64(0.565), 'MRR@5': np.float64(0.6157261904761905), 'MRR@10': np.float64(0.6220745464852608), 'Recall@5': 0.6985714285714286, 'Recall@10': 0.7478571428571429}\n"
     ]
    }
   ],
   "source": [
    "def compute_core_metrics(data, col_gold, col_pred, k=10):\n",
    "    \"\"\"\n",
    "    Compute MRR@{1,5,10} and Recall@{5,10}.\n",
    "    Any not-found document receives rank k+1 (never counted as a hit).\n",
    "    \"\"\"\n",
    "    ranks = data.apply(\n",
    "        lambda row:\n",
    "            (row[col_pred].index(row[col_gold]) + 1)\n",
    "            if row[col_gold] in row[col_pred]\n",
    "            else k + 1,       # ← always > k, so never a false hit\n",
    "        axis=1\n",
    "    ).to_numpy()\n",
    "\n",
    "    mrr1  = np.mean([1.0/r if r <= 1  else 0.0 for r in ranks])\n",
    "    mrr5  = np.mean([1.0/r if r <= 5  else 0.0 for r in ranks])\n",
    "    mrr10 = np.mean([1.0/r if r <= 10 else 0.0 for r in ranks])\n",
    "\n",
    "    recall5  = float((ranks <= 5 ).mean())\n",
    "    recall10 = float((ranks <= 10).mean())\n",
    "\n",
    "    return {\n",
    "        \"MRR@1\":  mrr1,\n",
    "        \"MRR@5\":  mrr5,\n",
    "        \"MRR@10\": mrr10,\n",
    "        \"Recall@5\":  recall5,\n",
    "        \"Recall@10\": recall10\n",
    "    }\n",
    "\n",
    "\n",
    "# Anwendung\n",
    "results_train = compute_core_metrics(df_query_train, 'cord_uid', 'bm25_topk')\n",
    "results_dev   = compute_core_metrics(df_query_dev,   'cord_uid', 'bm25_topk')\n",
    "\n",
    "print(\"Train:\", results_train)\n",
    "print(\"Dev:  \", results_dev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict on gold label test set and compute the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: {'MRR@1': np.float64(0.4384508990318119), 'MRR@5': np.float64(0.502524204702628), 'MRR@10': np.float64(0.509665415267075), 'Recall@5': 0.5982019363762102, 'Recall@10': 0.6507607192254495}\n"
     ]
    }
   ],
   "source": [
    "# Load  test set\n",
    "df_query_test = pd.read_csv(\n",
    "    \"../../data/subtask4b_query_tweets_test_gold.tsv\", \n",
    "    sep=\"\\t\", \n",
    "    dtype={\"post_id\": str, \"tweet_text\": str, \"cord_uid\": str}\n",
    ")\n",
    "\n",
    "# Retrieve BM25 top-10 for each test tweet\n",
    "df_query_test[\"bm25_topk\"] = df_query_test[\"tweet_text\"] \\\n",
    "    .apply(lambda txt: get_top_cord_uids(txt, k=10))\n",
    "\n",
    "# Compute core metrics on test\n",
    "results_test = compute_core_metrics(\n",
    "    df_query_test,\n",
    "    col_gold=\"cord_uid\",\n",
    "    col_pred=\"bm25_topk\"\n",
    ")\n",
    "\n",
    "print(\"Test:\", results_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Top 10 of BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRAIN\n",
    "# # Take top 10\n",
    "# df_query_train['preds'] = df_query_train['bm25_topk'].apply(lambda x: x[:10])\n",
    "\n",
    "# # Export\n",
    "# df_query_train[['post_id', 'preds']].to_csv('../predictions/predictions_BM25_Pre_Processed_train_TOP10.tsv', index=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEV\n",
    "# # Take top 10\n",
    "# df_query_dev['preds'] = df_query_dev['bm25_topk'].apply(lambda x: x[:10])\n",
    "\n",
    "# # Export\n",
    "# df_query_dev[['post_id', 'preds']].to_csv('../predictions/predictions_BM25_Pre_Processed_dev_TOP10.tsv', index=None, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST\n",
    "# # Take top 10\n",
    "# df_query_test['preds'] = df_query_test['bm25_topk'].apply(lambda x: x[:10])\n",
    "\n",
    "# # Export\n",
    "# df_query_test[['post_id', 'preds']].to_csv('../predictions/predictions_BM25_Pre_Processed_test_TOP10.tsv', index=None, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplementary baseline models: BM25f and TF-IDF \n",
    "_Note: BM25f and TF-IDF was not part of the official CLEF submission; it’s provided here for illustrative/experimental purposes._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extending the standard BM25 model to BM25F by treating the document's **title** and **abstract** as separate fields with different importance.  \n",
    "Each field is preprocessed (tokenized, lemmatized, lowercased, stopwords removed) using spaCy.  \n",
    "The BM25F model combines term frequencies across fields, applying field-specific weights and normalization to better model structured documents.\n",
    "\n",
    "- **Title weight**: 2.0\n",
    "- **Abstract weight**: 1.0\n",
    "- **Length normalization** applied per field\n",
    "- **IDF** computed across all fields combined\n",
    "\n",
    "During retrieval, queries are preprocessed similarly, and documents are ranked based on their BM25F scores.  \n",
    "Caching is used for faster repeated query lookup.\n",
    "\n",
    "This setup doesnt improve retrieval scores compared to BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Tokenize input text, lemmatize each token, lowercase it,\n",
    "    and filter out tokens that are either not pure words or numbers,\n",
    "    plus remove stopwords.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for tok in doc:\n",
    "        if (tok.is_alpha or tok.like_num) and not tok.is_stop:\n",
    "            tokens.append(tok.lemma_.lower())\n",
    "    return tokens\n",
    "\n",
    "# 1) Create two separate corpora: one for title and one for abstract\n",
    "title_corpus = df_collection['title'].tolist()\n",
    "abstract_corpus = df_collection['abstract'].tolist()\n",
    "cord_uids = df_collection['cord_uid'].tolist()\n",
    "\n",
    "# 2) Tokenize separately\n",
    "tokenized_title_corpus = [preprocess(doc) for doc in title_corpus]\n",
    "tokenized_abstract_corpus = [preprocess(doc) for doc in abstract_corpus]\n",
    "\n",
    "# 3) Calculate average field lengths\n",
    "avg_title_len = np.mean([len(doc) for doc in tokenized_title_corpus])\n",
    "avg_abstract_len = np.mean([len(doc) for doc in tokenized_abstract_corpus])\n",
    "\n",
    "# 4) Build BM25F class\n",
    "class BM25F:\n",
    "    def __init__(self, tokenized_titles, tokenized_abstracts, k1=1.5, b_title=0.75, b_abstract=0.75, w_title=2.0, w_abstract=1.0):\n",
    "        self.tokenized_titles = tokenized_titles\n",
    "        self.tokenized_abstracts = tokenized_abstracts\n",
    "        self.k1 = k1\n",
    "        self.b_title = b_title\n",
    "        self.b_abstract = b_abstract\n",
    "        self.w_title = w_title\n",
    "        self.w_abstract = w_abstract\n",
    "        \n",
    "        # Inverted index and stats\n",
    "        self.doc_count = len(tokenized_titles)\n",
    "        self.inverted_index_title = self._build_inverted_index(tokenized_titles)\n",
    "        self.inverted_index_abstract = self._build_inverted_index(tokenized_abstracts)\n",
    "        self.avg_title_len = avg_title_len\n",
    "        self.avg_abstract_len = avg_abstract_len\n",
    "    \n",
    "    def _build_inverted_index(self, corpus):\n",
    "        inverted = {}\n",
    "        for doc_id, tokens in enumerate(corpus):\n",
    "            for token in tokens:\n",
    "                if token not in inverted:\n",
    "                    inverted[token] = []\n",
    "                inverted[token].append(doc_id)\n",
    "        return inverted\n",
    "\n",
    "    def _idf(self, token):\n",
    "        df_title = len(set(self.inverted_index_title.get(token, [])))\n",
    "        df_abstract = len(set(self.inverted_index_abstract.get(token, [])))\n",
    "        df_total = df_title + df_abstract\n",
    "        if df_total == 0:\n",
    "            return 0\n",
    "        return np.log((self.doc_count - df_total + 0.5) / (df_total + 0.5) + 1)\n",
    "\n",
    "    def get_scores(self, query_tokens):\n",
    "        scores = np.zeros(self.doc_count)\n",
    "        for token in query_tokens:\n",
    "            idf = self._idf(token)\n",
    "            for doc_id in range(self.doc_count):\n",
    "                tf_title = self.tokenized_titles[doc_id].count(token)\n",
    "                tf_abstract = self.tokenized_abstracts[doc_id].count(token)\n",
    "                \n",
    "                len_title = len(self.tokenized_titles[doc_id])\n",
    "                len_abstract = len(self.tokenized_abstracts[doc_id])\n",
    "                \n",
    "                norm_title = tf_title / (1 - self.b_title + self.b_title * (len_title / self.avg_title_len)) if len_title else 0\n",
    "                norm_abstract = tf_abstract / (1 - self.b_abstract + self.b_abstract * (len_abstract / self.avg_abstract_len)) if len_abstract else 0\n",
    "                \n",
    "                tf_combined = self.w_title * norm_title + self.w_abstract * norm_abstract\n",
    "                \n",
    "                scores[doc_id] += idf * (tf_combined * (self.k1 + 1)) / (tf_combined + self.k1) if tf_combined > 0 else 0\n",
    "        return scores\n",
    "\n",
    "# 5) Initialize BM25F\n",
    "bm25f = BM25F(tokenized_title_corpus, tokenized_abstract_corpus)\n",
    "\n",
    "# 6) Update the retrieval function\n",
    "_text2bm25ftop = {}\n",
    "\n",
    "def get_top_cord_uids(query, k=5):\n",
    "    if query in _text2bm25ftop:\n",
    "        return _text2bm25ftop[query]\n",
    "    tokenized_query = preprocess(query)\n",
    "    doc_scores = bm25f.get_scores(tokenized_query)\n",
    "    indices = np.argsort(-doc_scores)[:k]\n",
    "    topk_uids = [cord_uids[i] for i in indices]\n",
    "    _text2bm25ftop[query] = topk_uids\n",
    "    return topk_uids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign BM25F top-k predictions\n",
    "df_query_train['bm25f_topk'] = df_query_train['tweet_text'].apply(lambda txt: get_top_cord_uids(txt, k=10))\n",
    "df_query_dev['bm25f_topk']   = df_query_dev['tweet_text'].apply(lambda txt: get_top_cord_uids(txt, k=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieved candidates using MRR@k\n",
    "def get_performance_mrr(data, col_gold, col_pred, list_k = [1, 5, 10]):\n",
    "    d_performance = {}\n",
    "    for k in list_k:\n",
    "        data[\"in_topx\"] = data.apply(lambda x: (1/([i for i in x[col_pred][:k]].index(x[col_gold]) + 1) if x[col_gold] in [i for i in x[col_pred][:k]] else 0), axis=1)\n",
    "        #performances.append(data[\"in_topx\"].mean())\n",
    "        d_performance[k] = data[\"in_topx\"].mean()\n",
    "    return d_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on train (BM25F): {1: np.float64(0.5724733525247024), 5: np.float64(0.6222879224046266), 10: np.float64(0.6282211613865702), 20: np.float64(0.6282211613865702), 50: np.float64(0.6282211613865702), 100: np.float64(0.6282211613865702)}\n",
      "Results on dev (BM25F): {1: np.float64(0.5778571428571428), 5: np.float64(0.6268095238095238), 10: np.float64(0.6322145691609976), 20: np.float64(0.6322145691609976), 50: np.float64(0.6322145691609976), 100: np.float64(0.6322145691609976)}\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print MRR for BM25F retrieval\n",
    "results_train = get_performance_mrr(df_query_train, 'cord_uid', 'bm25f_topk')\n",
    "results_dev   = get_performance_mrr(df_query_dev,   'cord_uid', 'bm25f_topk')\n",
    "\n",
    "print(f\"Results on train (BM25F): {results_train}\")\n",
    "print(f\"Results on dev (BM25F): {results_dev}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Top 10 of BM25F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEV\n",
    "# # Take top 10\n",
    "# df_query_dev['preds'] = df_query_dev['bm25f_topk'].apply(lambda x: x[:10])\n",
    "\n",
    "# # Export\n",
    "# df_query_dev[['post_id', 'preds']].to_csv('predictions_dev_BM25F_TOP10.tsv', index=None, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TRAIN\n",
    "# # Take top 10\n",
    "# df_query_train['preds'] = df_query_train['bm25f_topk'].apply(lambda x: x[:10])\n",
    "\n",
    "# # Export\n",
    "# df_query_train[['post_id', 'preds']].to_csv('predictions_train_BM25F_TOP10.tsv', index=None, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF–IDF  \n",
    "Represents each document as a TF–IDF vector. Each term’s weight is  \n",
    "$$\n",
    "\\log\\bigl(1 + \\mathrm{tf}_{t,d}\\bigr)\\;\\times\\;\\log\\frac{N}{\\mathrm{df}_{t}}\n",
    "$$  \n",
    "- `tf_{t,d}`: raw count of term *t* in document *d*  \n",
    "- `df_t`: number of documents containing *t*  \n",
    "- `N`: total number of documents  \n",
    "\n",
    "A tweet is vectorized in the same way, and we rank all documents by cosine similarity against the tweet vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF–IDF Results on train set: {1: np.float64(0.4935034622267175), 5: np.float64(0.5555434528903758), 10: np.float64(0.5555434528903758)}\n",
      "TF–IDF Results on dev   set: {1: np.float64(0.49357142857142855), 5: np.float64(0.5549761904761905), 10: np.float64(0.5549761904761905)}\n"
     ]
    }
   ],
   "source": [
    "# 1. Load collection\n",
    "collection = pd.read_pickle('../../data/subtask4b_collection_data.pkl')\n",
    "collection['text'] = collection['title'].fillna('') + ' ' + collection['abstract'].fillna('')\n",
    "docs = collection['text'].tolist()\n",
    "doc_ids = collection['cord_uid'].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=0.8, min_df=2)\n",
    "doc_tfidf = vectorizer.fit_transform(docs)\n",
    "\n",
    "# 2. Helper to generate top-5 run\n",
    "def get_tfidf_run(df):\n",
    "    rows=[]\n",
    "    for pid, text in zip(df['post_id'], df['tweet_text']):\n",
    "        sims = linear_kernel(vectorizer.transform([text]), doc_tfidf).flatten()\n",
    "        top5 = np.argsort(sims)[::-1][:5]\n",
    "        rows.append({'post_id': pid, 'preds': [doc_ids[i] for i in top5]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 3. Generate train+dev runs\n",
    "df_train_queries = pd.read_csv('../../data/subtask4b_query_tweets_train.tsv', sep='\\t')\n",
    "df_dev_queries   = pd.read_csv('../../data/subtask4b_query_tweets_dev.tsv',   sep='\\t')\n",
    "\n",
    "train_run = get_tfidf_run(df_train_queries)\n",
    "dev_run   = get_tfidf_run(df_dev_queries)\n",
    "\n",
    "train_run.to_csv('tfidf_train_run.tsv', sep='\\t', index=False)\n",
    "dev_run  .to_csv('tfidf_dev_run.tsv',   sep='\\t', index=False)\n",
    "\n",
    "# 4. Merge and parse preds\n",
    "df_train = df_train_queries.merge(train_run, on='post_id')\n",
    "df_train['tfidf_topk'] = df_train['preds']\n",
    "\n",
    "df_dev   = df_dev_queries  .merge(dev_run,   on='post_id')\n",
    "df_dev['tfidf_topk']  = df_dev  ['preds']\n",
    "\n",
    "# 5. Evaluate\n",
    "results_train = get_performance_mrr(df_train, 'cord_uid', 'tfidf_topk')\n",
    "results_dev   = get_performance_mrr(df_dev,   'cord_uid', 'tfidf_topk')\n",
    "\n",
    "print(f\"TF–IDF Results on train set: {results_train}\")\n",
    "print(f\"TF–IDF Results on dev   set: {results_dev}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Hyperparametertuning for BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k1=0.8, b=0.3 → MRR@5: 0.5220\n",
      "k1=0.8, b=0.5 → MRR@5: 0.5324\n",
      "k1=0.8, b=0.75 → MRR@5: 0.5406\n",
      "k1=0.8, b=0.9 → MRR@5: 0.5401\n",
      "k1=1.2, b=0.3 → MRR@5: 0.5191\n",
      "k1=1.2, b=0.5 → MRR@5: 0.5325\n",
      "k1=1.2, b=0.75 → MRR@5: 0.5399\n",
      "k1=1.2, b=0.9 → MRR@5: 0.5393\n",
      "k1=1.5, b=0.3 → MRR@5: 0.5139\n",
      "k1=1.5, b=0.5 → MRR@5: 0.5286\n",
      "k1=1.5, b=0.75 → MRR@5: 0.5353\n",
      "k1=1.5, b=0.9 → MRR@5: 0.5354\n",
      "k1=2.0, b=0.3 → MRR@5: 0.5069\n",
      "k1=2.0, b=0.5 → MRR@5: 0.5214\n",
      "k1=2.0, b=0.75 → MRR@5: 0.5309\n",
      "k1=2.0, b=0.9 → MRR@5: 0.5305\n"
     ]
    }
   ],
   "source": [
    "# Tookenize\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Tokenize the corpus\n",
    "corpus = df_collection['abstract'].fillna(\"\").apply(tokenize).tolist()\n",
    "\n",
    "# Set BM25 parameters\n",
    "k1_values = [0.8, 1.2, 1.5, 2.0]\n",
    "b_values = [0.3, 0.5, 0.75, 0.9]\n",
    "\n",
    "# Prepare queries and relevant documents\n",
    "queries = df_query_dev['tweet_text'].fillna(\"\").tolist()\n",
    "relevant_docs = df_query_dev['cord_uid'].tolist()\n",
    "\n",
    "# Initialize results list\n",
    "results = []\n",
    "\n",
    "for k1 in k1_values:\n",
    "    for b in b_values:\n",
    "        bm25 = BM25Okapi(corpus, k1=k1, b=b)\n",
    "\n",
    "        mrr_total = 0\n",
    "        for idx, query in enumerate(queries):\n",
    "            tokenized_query = tokenize(query)\n",
    "            scores = bm25.get_scores(tokenized_query)\n",
    "            ranked_indices = np.argsort(scores)[::-1]\n",
    "            ranked_cord_uids = df_collection.iloc[ranked_indices]['cord_uid'].tolist()\n",
    "\n",
    "            # Calculate MRR@5\n",
    "            try:\n",
    "                rank = ranked_cord_uids[:5].index(relevant_docs[idx]) + 1\n",
    "                mrr_total += 1 / rank\n",
    "            except ValueError:\n",
    "                continue  # If the relevant document is not found, skip\n",
    "\n",
    "        mrr_5 = mrr_total / len(queries)\n",
    "        results.append(((k1, b), mrr_5))\n",
    "        print(f\"k1={k1}, b={b} → MRR@5: {mrr_5:.4f}\")\n",
    "\n",
    "\n",
    "# default BM25Okapi(corpus, k1=1.5, b=0.75)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
