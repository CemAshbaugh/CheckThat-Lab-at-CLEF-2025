{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d9d8769",
   "metadata": {},
   "source": [
    "# Neural Re-ranking Pipeline  \n",
    "This Jupyter Notebook replicates the script for the MonoT5 and bi-encoder fallback re-ranking pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0642dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproducibility seeded at 42\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"Reproducibility seeded at {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debd5c5",
   "metadata": {},
   "source": [
    "## 1) Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48051876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MRR@1': 0.0, 'Recall@1': 0.0, 'MRR@5': 0.5, 'Recall@5': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_list_metrics(pred_lists, refs, ks=(1,5)):\n",
    "    ranks = []\n",
    "    for pred, gold in zip(pred_lists, refs):\n",
    "        try:\n",
    "            rank = next(i+1 for i, pid in enumerate(pred) if pid in gold)\n",
    "        except StopIteration:\n",
    "            rank = len(pred) + 1\n",
    "        ranks.append(rank)\n",
    "    ranks = np.array(ranks)\n",
    "    metrics = {}\n",
    "    for k in ks:\n",
    "        rr = [1.0/r if r <= k else 0.0 for r in ranks]\n",
    "        metrics[f\"MRR@{k}\"]    = float(np.mean(rr))\n",
    "        metrics[f\"Recall@{k}\"] = float((ranks <= k).mean())\n",
    "    return metrics\n",
    "\n",
    "# Example usage\n",
    "print(compute_list_metrics([[1,2,3],[4,5,6]], [[2],[5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad82c250",
   "metadata": {},
   "source": [
    "## 2) MonoT5-base cross-encoder rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d819cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "def score_pair(model, tokenizer, query, doc, device):\n",
    "    inp = f\"Query: {query} Document: {doc} Relevant:\"\n",
    "    tokens = tokenizer(inp, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    labels = tokenizer(\"true\", return_tensors=\"pt\").input_ids.to(device)\n",
    "    out = model(**tokens, labels=labels)\n",
    "    return -out.loss.item()\n",
    "\n",
    "def rerank_monot5(model, tokenizer, query, cands, id2text, device, top_k=5, batch_size=16):\n",
    "    scores = []\n",
    "    for i in range(0, min(len(cands), top_k), batch_size):\n",
    "        for pid in cands[i:i+batch_size]:\n",
    "            sc = score_pair(model, tokenizer, query, id2text[pid], device)\n",
    "            scores.append((pid, sc))\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [pid for pid, _ in scores[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4705c03c",
   "metadata": {},
   "source": [
    "## 3) Fallback candidates with bi-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21d6537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "\n",
    "def fallback_candidates(query, gold_id, bi_encoder, paper_emb, paper_ids, device, top_k=5):\n",
    "    q_emb = bi_encoder.encode(query, convert_to_tensor=True, normalize_embeddings=True).to(device)\n",
    "    cos_scores = util.cos_sim(q_emb, paper_emb)[0]\n",
    "    top_idxs = torch.topk(cos_scores, k=top_k).indices.cpu().tolist()\n",
    "    cands = [paper_ids[i] for i in top_idxs]\n",
    "    if gold_id not in cands:\n",
    "        cands.append(gold_id)\n",
    "    return cands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58089a9",
   "metadata": {},
   "source": [
    "## 4) MonoT5-3B batched rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0cbd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rerank_batched(model, tokenizer, query, cands, id2text, device, top_k=5, batch_size=16, alpha=0.7):\n",
    "    window = cands[:top_k]\n",
    "    inputs = [f\"Query: {query} Document: {id2text[pid]} Relevant:\" for pid in window]\n",
    "    raw_scores = []\n",
    "    for i in range(0, len(inputs), batch_size):\n",
    "        batch_in = inputs[i:i+batch_size]\n",
    "        enc = tokenizer(batch_in, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
    "        labs = tokenizer([\"true\"]*len(batch_in), return_tensors=\"pt\", padding=True).input_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(**enc, labels=labs)\n",
    "        batch_score = -out.loss.item()\n",
    "        raw_scores.extend([batch_score]*len(batch_in))\n",
    "    arr = np.array(raw_scores)\n",
    "    mn, mx = arr.min(), arr.max()\n",
    "    norm = (arr - mn)/(mx - mn + 1e-8)\n",
    "    base = np.array([1 - (i/(len(window)-1)) for i in range(len(window))])\n",
    "    final = alpha*norm + (1-alpha)*base\n",
    "    scored = list(zip(window, final))\n",
    "    scored.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [pid for pid, _ in scored[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95fc925",
   "metadata": {},
   "source": [
    "## 5) Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    # Paths & data\n",
    "    base_dir = Path.cwd().parent.parent\n",
    "    data_dir = Path(os.getenv(\"DATA_DIR\", base_dir/\"data\"))\n",
    "    col_path = next(data_dir.rglob(\"subtask4b_collection_data.pkl\"))\n",
    "    train_preds_path = Path(\"../../predictions/Neural_Representation_Learning/E5_base_setup_train_TOP100.tsv\")\n",
    "    dev_preds_path   = Path(\"../../predictions/Neural_Representation_Learning/E5_base_setup_dev_TOP100.tsv\")\n",
    "\n",
    "    # Load collection\n",
    "    papers_df = pd.read_pickle(col_path)\n",
    "    papers_df['text'] = papers_df.apply(lambda r: (r.get('title','') + ' ' + r.get('abstract','')).strip(), axis=1)\n",
    "    id2text   = dict(zip(papers_df['cord_uid'], papers_df['text']))\n",
    "    paper_ids = papers_df['cord_uid'].tolist()\n",
    "\n",
    "    # Load queries & preds\n",
    "    train_df   = pd.read_csv(next(data_dir.rglob(\"subtask4b_query_tweets_train.tsv\")), sep='\\t')\n",
    "    dev_df     = pd.read_csv(next(data_dir.rglob(\"subtask4b_query_tweets_dev.tsv\")),   sep='\\t')\n",
    "    pred_train = pd.read_csv(train_preds_path, sep='\\t', index_col='post_id')\n",
    "    pred_dev   = pd.read_csv(dev_preds_path,   sep='\\t', index_col='post_id')\n",
    "    pred_col   = 'preds' if 'preds' in pred_dev.columns else pred_dev.columns[0]\n",
    "\n",
    "    # Setup fallback encoder\n",
    "    bi_encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n",
    "    paper_emb  = bi_encoder.encode(papers_df['text'].tolist(), convert_to_tensor=True, normalize_embeddings=True).to(device)\n",
    "\n",
    "    # Load models\n",
    "    m1_name = \"castorini/monot5-base-msmarco-10k\"\n",
    "    tok1 = T5TokenizerFast.from_pretrained(m1_name)\n",
    "    m1   = T5ForConditionalGeneration.from_pretrained(m1_name).to(device)\n",
    "    m1.eval()\n",
    "    m2_name = \"castorini/monot5-3b-msmarco\"\n",
    "    tok2 = T5TokenizerFast.from_pretrained(m2_name)\n",
    "    m2   = T5ForConditionalGeneration.from_pretrained(m2_name).to(device)\n",
    "    m2.eval()\n",
    "\n",
    "    # Evaluate both splits\n",
    "    for split_name, df, pred_df in [(\"Train\", train_df, pred_train), (\"Dev\", dev_df, pred_dev)]:\n",
    "        refs = [[row['cord_uid']] for _, row in df.iterrows()]\n",
    "        print(f\"\\n=== Split: {split_name} ===\")\n",
    "\n",
    "        # MonoT5-base\n",
    "        preds1 = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Base rerank\"):\n",
    "            qid = row['post_id']\n",
    "            cands = ast.literal_eval(pred_df.at[qid, pred_col])[:5]\n",
    "            preds1.append(rerank_monot5(m1, tok1, row['tweet_text'], cands, id2text, device))\n",
    "        metrics1 = compute_list_metrics(preds1, refs, ks=(1,5))\n",
    "        print(f\"MRR@1: {metrics1['MRR@1']:.4f}, MRR@5: {metrics1['MRR@5']:.4f}, Recall@5: {metrics1['Recall@5']:.4f}\")\n",
    "\n",
    "        # MonoT5-3B batched\n",
    "        preds2 = []\n",
    "        for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Batched rerank\"):\n",
    "            qid, gold = row['post_id'], row['cord_uid']\n",
    "            cands = ast.literal_eval(pred_df.at[qid, pred_col])\n",
    "            if gold not in cands:\n",
    "                cands = fallback_candidates(row['tweet_text'], gold, bi_encoder, paper_emb, paper_ids, device)\n",
    "            preds2.append(rerank_batched(m2, tok2, row['tweet_text'], cands, id2text, device))\n",
    "        metrics2 = compute_list_metrics(preds2, refs, ks=(1,5))\n",
    "        print(f\"MRR@1: {metrics2['MRR@1']:.4f}, MRR@5: {metrics2['MRR@5']:.4f}, Recall@5: {metrics2['Recall@5']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc55b7b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
